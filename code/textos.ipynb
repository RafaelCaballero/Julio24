{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "506fd5ae",
      "metadata": {
        "id": "506fd5ae"
      },
      "source": [
        "\n",
        "# Análisis de textos\n",
        "\n",
        "El mundo del análisis de textos es totalmente diferente a lo visto hasta ahora, con sus propios problemas y bibliotecas especializadas. Vamos a ver solo algunas de sus características.\n",
        "\n",
        "Empecemos asegurándonos de que las bibliotecas necesarias están instaladas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df248cb8",
      "metadata": {
        "id": "df248cb8"
      },
      "outputs": [],
      "source": [
        "modules = [\"wikipedia\",  \"bs4\", \"spacy\", \"nltk\",\"sentiment_analysis_spanish\",\"flair\"]\n",
        "\n",
        "\n",
        "import sys\n",
        "import os.path\n",
        "from subprocess import check_call\n",
        "import importlib\n",
        "import os\n",
        "\n",
        "def instala(modules):\n",
        "  print(\"Instalando módulos\")\n",
        "  for m in modules:\n",
        "      # para el import quitamos [...] y ==...\n",
        "      p = m.find(\"[\")\n",
        "      mi = m if p==-1 else m[:p]\n",
        "      p = mi.find(\"==\")\n",
        "      mi = mi if p==-1 else mi[:p]\n",
        "\n",
        "      torch_loader = importlib.util.find_spec(mi)\n",
        "      if torch_loader is not None:\n",
        "          print(m,\" encontrado\")\n",
        "      else:\n",
        "          print(m,\" No encontrado, instalando...\",end=\"\")\n",
        "          try:\n",
        "            r = check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--user\", m])\n",
        "            print(\"¡hecho!\")\n",
        "          except:\n",
        "            print(\"¡Problema al instalar \",m,\"! ¿seguro que el módulo existe?\",sep=\"\")\n",
        "\n",
        "  print(\"¡Terminado!\")\n",
        "\n",
        "instala(modules)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25ec65ad",
      "metadata": {
        "id": "25ec65ad"
      },
      "source": [
        "## Tokens\n",
        "\n",
        "La primera idea es obtener las piezas sintácticas básicas, palabras, símbolos de puntuación, etc., pero esto no es tan trivial como parece. Empezamos usando la función estándar `split`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47832b15",
      "metadata": {
        "id": "47832b15"
      },
      "outputs": [],
      "source": [
        "s = \"Desde lo alto se divisa la ciudad y toda la campiña\"\n",
        "l = s.split(\" \")\n",
        "l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f38f004",
      "metadata": {
        "id": "6f38f004"
      },
      "outputs": [],
      "source": [
        "s = \"Se remueve, levanta una tenue polvareda, avanza.\"\n",
        "l = s.split(\" \")\n",
        "l"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc9aeaa6",
      "metadata": {
        "id": "dc9aeaa6"
      },
      "source": [
        "Bibliotecas como spaCy permiten obtener mejor los tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8866feda",
      "metadata": {
        "id": "8866feda"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.blank('es') # modelo\n",
        "s = \"Se remueve, levanta una tenue polvareda, avanza.\"\n",
        "doc =  nlp(s)\n",
        "l = [token.text for token in doc]\n",
        "l"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e16454f6",
      "metadata": {
        "id": "e16454f6"
      },
      "source": [
        "Podemos \"limpiar\" el texto quedando solo con un tipo de palabras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08c1c50d",
      "metadata": {
        "id": "08c1c50d"
      },
      "outputs": [],
      "source": [
        "l =  [token.text for token in doc if  token.is_alpha]\n",
        "l"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c75c5786",
      "metadata": {
        "id": "c75c5786"
      },
      "source": [
        "Un caso particular de *token* son las stop words, palabras sin semántica que pueden desvirtuar nuestros análisis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a500b028",
      "metadata": {
        "id": "a500b028"
      },
      "outputs": [],
      "source": [
        "l =  [token.text for token in doc if  token.is_alpha and not token.is_stop]\n",
        "l"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b271a62",
      "metadata": {
        "id": "9b271a62"
      },
      "source": [
        "### Contando oraciones y palabras\n",
        "\n",
        "Igual que con las palabras podemos intentarlo desde Python estándar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afb0fa3a",
      "metadata": {
        "id": "afb0fa3a"
      },
      "outputs": [],
      "source": [
        "s = \"he comprado patatas, naranjas, manzanas...\"\n",
        "punto = s.find(\".\") # posición del primer punto\n",
        "if punto!=-1: # -1 si no hay puntos\n",
        "  print(\"La primera oración: \", s[:punto])\n",
        "  print(\"El resto del texto: \", s[punto+1:])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d83d8d1e",
      "metadata": {
        "id": "d83d8d1e"
      },
      "source": [
        "Spacy tiene \"modelos pre-entrenados\". Descargamos uno de ellos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91d54941",
      "metadata": {
        "id": "91d54941"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download es_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6107e54",
      "metadata": {
        "id": "e6107e54"
      },
      "outputs": [],
      "source": [
        "import es_core_news_sm\n",
        "nlp = es_core_news_sm.load()\n",
        "s = \"he comprado patatas, naranjas, manzanas... todo en \\\n",
        "     http://verdurasfrescas.com. ¡Y ha llegado muy rápido!\"\n",
        "doc = nlp(s)\n",
        "for i,o in enumerate(doc.sents):\n",
        "    print(i+1,o)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79e36e7a",
      "metadata": {
        "id": "79e36e7a"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "pagina = \"https://www.gutenberg.org/cache/epub/8870/pg8870.html\"\n",
        "page = requests.get(pagina)\n",
        "soup = BeautifulSoup(page.text, 'html.parser')  # le pasamos el texto en HTML para que lo analice\n",
        "soup.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9469f0a6",
      "metadata": {
        "id": "9469f0a6"
      },
      "outputs": [],
      "source": [
        "doc =  nlp(soup.text)\n",
        "print(\"El libro incluye \", len(list(doc.sents)), \"oraciones\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f0851ea",
      "metadata": {
        "id": "5f0851ea"
      },
      "source": [
        "Si lo que queremos es contar palabras podemos utilizar `Counter` que incluye un diccionario con cada letra de la palabra y su frecuencia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01dcef4f",
      "metadata": {
        "id": "01dcef4f"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "palabras =  [token.text for token in doc if  token.is_alpha and not token.is_stop]\n",
        "c = Counter(palabras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70d7693c",
      "metadata": {
        "id": "70d7693c"
      },
      "outputs": [],
      "source": [
        "c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a69860e1",
      "metadata": {
        "id": "a69860e1"
      },
      "outputs": [],
      "source": [
        "c.most_common(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebb7b773",
      "metadata": {
        "id": "ebb7b773"
      },
      "source": [
        "Podemos añadir \"the\" como stopword"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "446a84ab",
      "metadata": {
        "id": "446a84ab"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.blank(\"es\")\n",
        "nlp.Defaults.stop_words.add(\"the\")\n",
        "doc =  nlp(soup.text)\n",
        "palabras =  [token.text for token in doc if  token.is_alpha and not token.is_stop]\n",
        "c = Counter(palabras)\n",
        "c.most_common(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87accf3b",
      "metadata": {
        "id": "87accf3b"
      },
      "source": [
        "### Lemas y raíces\n",
        "\n",
        "El resultado anterior se ve afectado por el hecho de que la misma palabra puede aparecer en singular, plural, verbos conjugados de distinta forma, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "946b9c8a",
      "metadata": {
        "id": "946b9c8a"
      },
      "outputs": [],
      "source": [
        "import es_core_news_sm\n",
        "nlp = es_core_news_sm.load()\n",
        "doc =  nlp(soup.text)\n",
        "palabras = [p.lemma_.upper() for p in doc\n",
        "            if p.is_alpha and not p.is_stop]\n",
        "c = Counter(palabras)\n",
        "c.most_common(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea582dac",
      "metadata": {
        "id": "ea582dac"
      },
      "source": [
        "La **extracción de raíces**, conocida en la literatura en inglés como *stemming*, que convierte palabras a su raíz eliminando sufijos de persona, género, etc. Es una alternativa a la lematización que ha sido tradicionalmente utilizada en aplicaciones de inteligencia artificial, por ejemplo en *aprendizaje automático*, pero que no es tan útil para nuestros propósitos al no obtenerse palabras completas como resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ea6cfd4",
      "metadata": {
        "id": "1ea6cfd4"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "stemmer = SnowballStemmer('spanish')\n",
        "palabras = [stemmer.stem(p.text).upper() for p in doc\n",
        "              if p.is_alpha and not p.is_stop]\n",
        "frecuencia = Counter(palabras)\n",
        "palabras_comunes = frecuencia.most_common(5)\n",
        "\n",
        "palabras_comunes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1a89a0e",
      "metadata": {
        "id": "e1a89a0e"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "import es_core_news_sm\n",
        "nlp = es_core_news_sm.load()\n",
        "doc = nlp(soup.text)\n",
        "\n",
        "stemmer = SnowballStemmer('spanish')\n",
        "\n",
        "palabras = [p.text.upper() for p in doc\n",
        "             if p.is_alpha and not p.is_stop]\n",
        "print(\"Palabras no vacías en el texto \", len(set(palabras)))\n",
        "\n",
        "palabras = [p.lemma_.upper() for p in doc\n",
        "            if p.is_alpha and not p.is_stop]\n",
        "print(\"Con lemas \", len(set(palabras)))\n",
        "palabras = [stemmer.stem(p.text).upper() for p in doc\n",
        "              if p.is_alpha and not p.is_stop]\n",
        "print(\"Con raíces \", len(set(palabras)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ab57cf3",
      "metadata": {
        "id": "3ab57cf3"
      },
      "source": [
        "## Análisis de sentimiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c67020c9",
      "metadata": {
        "id": "c67020c9"
      },
      "outputs": [],
      "source": [
        "# origen https://www.amazon.com/-/es/product-reviews/B07ZPKF8RG/ref=cm_cr_unknown?ie=UTF8&filterByStar=two_star&reviewerType=all_reviews&pageNumber=1#reviews-filter-bar\n",
        "textos = ['Broken. It was supposed to be a B-Day gift for my mom and now she doesn’t get a gift on her Bday',\n",
        "          'Touchscreen became almost completely unresponsive over 20% of surface area within days',\n",
        "          \"A little over 90 days, hardware failure, NO Solution, DON'T BUY\",\n",
        "          \"Wanted to love it, but it had too many problems upon arrival\",\n",
        "          \"Phone is great but the battery health is 76%\",\n",
        "          \"Happy customer! It came with protective glass installed.\",\n",
        "          \"Great phone for the price!\",\n",
        "          \"I love this phone\",\n",
        "          \"Definitely Recommend!\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45264afb",
      "metadata": {
        "id": "45264afb"
      },
      "outputs": [],
      "source": [
        "from flair.models import TextClassifier\n",
        "from flair.data import Sentence\n",
        "classifier = TextClassifier.load('en-sentiment')\n",
        "\n",
        "for s in textos:\n",
        "    text = Sentence(s)\n",
        "    classifier.predict(text)\n",
        "    value = text.labels[0].to_dict()\n",
        "    print(s)\n",
        "    print(value)\n",
        "    print(\"=\"*30)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e528f9e",
      "metadata": {
        "id": "9e528f9e"
      },
      "source": [
        "Aplicándoselo a una columna de Pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c43dfd52",
      "metadata": {
        "id": "c43dfd52"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"https://github.com/RafaelCaballero/tdm/blob/master/datos/IMDB10K.zip?raw=true\",compression='zip')\n",
        "total = 250\n",
        "df = df.sample(250)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae2732a1",
      "metadata": {
        "id": "ae2732a1"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "flairOp = []\n",
        "for s in tqdm(df.review):\n",
        "    text = Sentence(s)\n",
        "    classifier.predict(text)\n",
        "    value = text.labels[0].to_dict()[\"value\"]\n",
        "    flairOp.append(+1 if value==\"POSITIVE\" else 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b548aef2",
      "metadata": {
        "id": "b548aef2"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_real = [+1 if v==\"positive\" else 0 for v in df.sentiment]\n",
        "\n",
        "cm = confusion_matrix(y_real, flairOp)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "516dcbb2",
      "metadata": {
        "id": "516dcbb2"
      },
      "source": [
        "## NER (Named Entity Recognition)\n",
        "\n",
        "Reconocimiento de sustantivos que denotan nombres, organizaciones, etc. Importante para saber de qué se está hablando en textos complejos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da61d12e",
      "metadata": {
        "id": "da61d12e"
      },
      "outputs": [],
      "source": [
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "# load tagger\n",
        "tagger = SequenceTagger.load(\"flair/ner-spanish-large\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e42751e",
      "metadata": {
        "id": "1e42751e"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# make example sentence\n",
        "sentence = Sentence(\"Belén fue a Belén el mismo día que Santiago llegó a Santiago de Compostela\")\n",
        "\n",
        "# predict NER tags\n",
        "tagger.predict(sentence)\n",
        "\n",
        "# print sentence\n",
        "print(sentence)\n",
        "\n",
        "# print predicted NER spans\n",
        "print('The following NER tags are found:')\n",
        "# iterate over entities and print\n",
        "for entity in sentence.get_spans('ner'):\n",
        "    print(entity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48beafa3",
      "metadata": {
        "id": "48beafa3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}