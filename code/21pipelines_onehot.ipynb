{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RafaelCaballero/Julio24/blob/main/code/21pipelines_onehot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db78748d",
      "metadata": {
        "id": "db78748d"
      },
      "source": [
        "# Introducción a la ciencia de datos con Python\n",
        "Rafa Caballero\n",
        "\n",
        "\n",
        "\n",
        "# Pipelines\n",
        "\n",
        "Empezamos con un ejemplo, son datos de clientes de bancos y si han abandonado el banco (churn) tras un número de meses o no.\n",
        "\n",
        "Los datos están tomados de [aquí](https://www.kaggle.com/datasets/gauravtopre/bank-customer-churn-dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3816655d",
      "metadata": {
        "id": "3816655d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/RafaelCaballero/tdm/master/datos/BankPrediction.csv\"\n",
        "df = pd.read_csv(url)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43b60f7e",
      "metadata": {
        "id": "43b60f7e"
      },
      "source": [
        "### 1 Limpieza y análisis exploratorio"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b9e4f79",
      "metadata": {
        "id": "5b9e4f79"
      },
      "source": [
        "**Ejercicio** ¿Cuántos valores diferentes toma customer_id? ¿qué te sugiere eso?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb5c4b08",
      "metadata": {
        "id": "bb5c4b08"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88069ed1",
      "metadata": {
        "id": "88069ed1"
      },
      "outputs": [],
      "source": [
        "# no nos permiten usar datos personales\n",
        "df2 = df.drop(columns=[\"customer_id\"])\n",
        "df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab5f4312",
      "metadata": {
        "id": "ab5f4312"
      },
      "outputs": [],
      "source": [
        "df2.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae8cebdf",
      "metadata": {
        "id": "ae8cebdf"
      },
      "source": [
        "Vemos que hay dos `object` que normalmente corresponden a  tipo string. Veamos primero `gender`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2537672f",
      "metadata": {
        "id": "2537672f"
      },
      "outputs": [],
      "source": [
        "df2.gender.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57af8798",
      "metadata": {
        "id": "57af8798"
      },
      "source": [
        "como solo son 2 valores podemos dejarlo en una sola columna con valores 0,1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52ca10f8",
      "metadata": {
        "id": "52ca10f8"
      },
      "outputs": [],
      "source": [
        "df2[\"genderb\"] = 1\n",
        "df2.loc[df2.gender==\"Male\", \"genderb\"] = 0\n",
        "df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e128411d",
      "metadata": {
        "id": "e128411d"
      },
      "outputs": [],
      "source": [
        "df3 = df2.copy()\n",
        "df3 = df2.drop(columns=[\"gender\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "710b067b",
      "metadata": {
        "id": "710b067b"
      },
      "source": [
        "En el caso de la columna de los países lo mejor es aplicar one-hot encoding. Una forma sencilla es utilizar el método `get_dummies`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f738a3af",
      "metadata": {
        "id": "f738a3af"
      },
      "outputs": [],
      "source": [
        "df4 = pd.get_dummies(df3)\n",
        "df4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8ec4e00",
      "metadata": {
        "id": "d8ec4e00"
      },
      "source": [
        "Es cómo mirar los datos numéricos por separado, por eso usamos `df2`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a836499",
      "metadata": {
        "id": "0a836499"
      },
      "outputs": [],
      "source": [
        "df2.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9972cdb",
      "metadata": {
        "id": "d9972cdb"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "correlaciones = df2.corr(numeric_only = True)\n",
        "sns.clustermap(correlaciones,\n",
        "                   method = 'complete',\n",
        "                   cmap   = 'RdBu',\n",
        "                   annot  = True,\n",
        "                   annot_kws = {'size': 8})\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3e508b8",
      "metadata": {
        "id": "f3e508b8"
      },
      "source": [
        "Podríamos haber utilizado el dataframe tras el one-hot encoding, pero normalmente no aporta mucha información"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9ef564f",
      "metadata": {
        "id": "f9ef564f"
      },
      "outputs": [],
      "source": [
        "correlaciones = df4.corr()\n",
        "sns.clustermap(correlaciones,\n",
        "                   method = 'complete',\n",
        "                   cmap   = 'RdBu',\n",
        "                   annot  = True,\n",
        "                   annot_kws = {'size': 8})\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfff5ad4",
      "metadata": {
        "id": "dfff5ad4"
      },
      "source": [
        "Histogramas y boxplots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dba2ada3",
      "metadata": {
        "id": "dba2ada3"
      },
      "outputs": [],
      "source": [
        "for c in df2.columns:\n",
        "    fig, [ax1,ax2] = plt.subplots(1,2,figsize=(10, 5))\n",
        "    df2[c].hist(ax=ax1)\n",
        "    if np.issubdtype(df2[c].dtype, np.number):\n",
        "        df2.boxplot(column=c,ax=ax2)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3330039",
      "metadata": {
        "id": "b3330039"
      },
      "source": [
        "## Primer modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21da0db4",
      "metadata": {
        "id": "21da0db4"
      },
      "source": [
        "Parece que nos puede interesar escalar y también hacer oversampling o similar. Primero lo vamos a hacer sin nada de esto para obtener una estimación inicial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "092c3d1b",
      "metadata": {
        "id": "092c3d1b"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# 1 dividir columnas\n",
        "yColumn = \"churn\"\n",
        "XColumns = [c for c in df4.columns if c!=yColumn]\n",
        "X = df4[XColumns]\n",
        "y = df4[yColumn]\n",
        "\n",
        "# 2 preparar train y test\n",
        "test = 0.4\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= test)\n",
        "\n",
        "# 3 método y entrenamiento\n",
        "metodo = LogisticRegression()\n",
        "modelo = metodo.fit(X_train,y_train)\n",
        "\n",
        "# 4 evaluar\n",
        "y_pred = modelo.predict(X_test)\n",
        "k =  cohen_kappa_score(y_test,y_pred)\n",
        "print(\"kappa \",k)\n",
        "cm = confusion_matrix(y_test, y_pred, labels=modelo.classes_)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=modelo.classes_)\n",
        "disp.plot()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "864df23a",
      "metadata": {
        "id": "864df23a"
      },
      "source": [
        "El resultado es prácticamente nulo, parece que apenas hay información útil... Vamos a añadir primero el escalado ¿dónde ponerlo?\n",
        "El sitio es entre el paso 2 y el 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54159d5f",
      "metadata": {
        "id": "54159d5f"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
        "\n",
        "# 1\n",
        "# ....\n",
        "\n",
        "# 2\n",
        "# ...\n",
        "\n",
        "# y ahora escalamos, solo podemos usar los datos de x_train para \"entrenar\"\n",
        "# OJO: solo se escalan las X, no las ys\n",
        "escalador = StandardScaler()\n",
        "escalador.fit(X_train)\n",
        "X_traine = escalador.transform(X_train)\n",
        "X_teste = escalador.transform(X_test)\n",
        "\n",
        "# 3 método y entrenamiento\n",
        "metodo = LogisticRegression()\n",
        "modelo = metodo.fit(X_traine,y_train)\n",
        "\n",
        "# 4 evaluar\n",
        "y_pred = modelo.predict(X_teste)\n",
        "k =  cohen_kappa_score(y_test,y_pred)\n",
        "print(\"kappa \",k)\n",
        "cm = confusion_matrix(y_test, y_pred, labels=modelo.classes_)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=modelo.classes_)\n",
        "disp.plot()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd4abf4e",
      "metadata": {
        "id": "fd4abf4e"
      },
      "source": [
        "Ideas importantes:\n",
        "\n",
        "- solo se escalan las X, nunca las y\n",
        "- Se aprende con x_train, pero el escalador debe afectar tanto a X_train como a X_test\n",
        "\n",
        "Pegas: ¿cómo hacerlo en el caso de cross validation? ¿Cómo añadir el oversampler?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d9bdc2f",
      "metadata": {
        "id": "6d9bdc2f"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "# 1 dividir columnas\n",
        "yColumn = \"churn\"\n",
        "XColumns = [c for c in df4.columns if c!=yColumn]\n",
        "X = df4[XColumns]\n",
        "y = df4[yColumn]\n",
        "\n",
        "# 2 preparar train y test\n",
        "test = 0.4\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= test)\n",
        "\n",
        "# 3 método y entrenamiento\n",
        "steps = [('scaler', StandardScaler()), ('Logistic', LogisticRegression())]\n",
        "metodo = Pipeline(steps)\n",
        "modelo = metodo.fit(X_train,y_train)\n",
        "\n",
        "# 4 evaluar\n",
        "y_pred = modelo.predict(X_test)\n",
        "k =  cohen_kappa_score(y_test,y_pred)\n",
        "print(\"kappa \",k)\n",
        "cm = confusion_matrix(y_test, y_pred, labels=modelo.classes_)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=modelo.classes_)\n",
        "disp.plot()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23eac76b",
      "metadata": {
        "id": "23eac76b"
      },
      "source": [
        "La idea es que el escalador queda \"pegado\" al método, de forma forma solo se hace fit del train y se aplica el escalador entrenado al test.\n",
        "\n",
        "Un detalle técnico: al utilizar SMOTE, RandomOverSampler, etc. debemos usar la clase Pipeline de la clase imblearn\n",
        "\n",
        "**Ejercicio** Añadir SMOTE(), RandomOverSampler(), RandomUnderSampler()....lo que se quiera en el pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60952e54",
      "metadata": {
        "id": "60952e54"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "steps = [('scaler', StandardScaler()),  ('Logistic', LogisticRegression())]\n",
        "metodo = Pipeline(steps)\n",
        "modelo = metodo.fit(X_train,y_train)\n",
        "\n",
        "# 4 evaluar\n",
        "y_pred = modelo.predict(X_test)\n",
        "k =  cohen_kappa_score(y_test,y_pred)\n",
        "print(\"kappa \",k)\n",
        "cm = confusion_matrix(y_test, y_pred, labels=modelo.classes_)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=modelo.classes_)\n",
        "disp.plot()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fd75152",
      "metadata": {
        "id": "1fd75152"
      },
      "source": [
        "Buscando el mejor punto de corte"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a87cb58",
      "metadata": {
        "id": "6a87cb58"
      },
      "outputs": [],
      "source": [
        "# 4 evaluar\n",
        "y_probs = modelo.predict_proba(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e037d47",
      "metadata": {
        "id": "5e037d47"
      },
      "outputs": [],
      "source": [
        "# 4 evaluar\n",
        "xs=[]\n",
        "ys=[]\n",
        "for cut in [0.1,0.2,0.3,0.4,0.425,0.45,0.475,0.5,0.525,0.55,0.575,0.6,0.7,0.8,0.9]:\n",
        "    y_pred = [1 if p[1]>cut else 0 for p in y_probs]\n",
        "    k =  cohen_kappa_score(y_test,y_pred)\n",
        "    print(cut,k)\n",
        "    xs.append(cut)\n",
        "    ys.append(k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa9c445a",
      "metadata": {
        "id": "aa9c445a"
      },
      "outputs": [],
      "source": [
        "plt.plot(xs,ys)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ce5833b",
      "metadata": {
        "id": "7ce5833b"
      },
      "outputs": [],
      "source": [
        "cut = 0.3\n",
        "y_pred = [1 if p[1]>cut else 0 for p in y_probs]\n",
        "k =  cohen_kappa_score(y_test,y_pred)\n",
        "print(\"kappa \",k)\n",
        "cm = confusion_matrix(y_test, y_pred, labels=modelo.classes_)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=modelo.classes_)\n",
        "disp.plot()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91d36a88",
      "metadata": {
        "id": "91d36a88"
      },
      "source": [
        "El método también vale para validación cruzada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6880bc50",
      "metadata": {
        "id": "6880bc50"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold,cross_val_score\n",
        "\n",
        "steps = [('smote',SMOTE()), ('scaler', StandardScaler()),  ('Logistic', LogisticRegression())]\n",
        "scorer = make_scorer(cohen_kappa_score)\n",
        "pipeline = Pipeline(steps=steps)\n",
        "cv = RepeatedStratifiedKFold(n_splits=20, n_repeats=5)\n",
        "scores = cross_val_score(pipeline, X, y, scoring=scorer, cv=cv)\n",
        "scores.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e750ef6b",
      "metadata": {
        "id": "e750ef6b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}